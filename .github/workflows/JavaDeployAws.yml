name: Java CI with Maven

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

env:
  CLUSTER_NAME: EKS-QUICK-SERVE1
  AWS_REGION: us-east-1
  NODEGROUP_NAME: quick-serve
  VPC_ID: vpc-08325c03e3c0c4699
  SG_NAME: eks-cluster-sg

jobs:
  build-docker-image:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up JDK 17
        uses: actions/setup-java@v3
        with:
          java-version: '17'
          distribution: 'temurin'

      - name: Check Java Version
        run: java -version

      - name: Check Maven Java Version
        run: mvn -v

      - name: Log in to Docker Hub
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKER_HUB_USERNAME }}
          password: ${{ secrets.DOCKER_HUB_ACCESS_TOKEN }}

      - name: Build Docker Image
        run: |
          IMAGE_NAME=quick-serve-api-pedido:1.0
          docker build -t $IMAGE_NAME .
          echo "IMAGE_NAME=$IMAGE_NAME" >> $GITHUB_ENV
      - name: Push Docker Image to Docker Hub
        run: |
          docker tag $IMAGE_NAME ${{ secrets.DOCKER_HUB_USERNAME }}/$IMAGE_NAME
          docker push ${{ secrets.DOCKER_HUB_USERNAME }}/$IMAGE_NAME

  create-cluster:
    runs-on: ubuntu-latest
    needs: build-docker-image
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}

      - name: Check if Security Group Exists
        id: check-sg
        run: |
          SG_ID=$(aws ec2 describe-security-groups \
            --filters Name=vpc-id,Values=${{ env.VPC_ID }} Name=group-name,Values=${{ env.SG_NAME }} \
            --query "SecurityGroups[0].GroupId" \
            --output text 2>/dev/null || echo "null")
          if [ "$SG_ID" != "null" ]; then
            echo "Security Group already exists: $SG_ID"
            echo "SG_ID=$SG_ID" >> $GITHUB_ENV
          else
            echo "Security Group does not exist."
            echo "SG_ID=" >> $GITHUB_ENV
          fi

      - name: Create Security Group
        if: env.SG_ID == ''
        run: |
          SG_ID=$(aws ec2 create-security-group \
            --group-name ${{ env.SG_NAME }} \
            --description "Security Group for EKS Cluster" \
            --vpc-id ${{ env.VPC_ID }} \
            --query "GroupId" \
            --output text)
          echo "Created Security Group: $SG_ID"
          echo "SG_ID=$SG_ID" >> $GITHUB_ENV

      - name: Check and Authorize Security Group Ingress
        run: |
          # Verificar se a regra jÃ¡ existe
          RULE_EXISTS=$(aws ec2 describe-security-groups \
            --group-ids ${{ env.SG_ID }} \
            --query "SecurityGroups[0].IpPermissions[?IpRanges[?CidrIp=='0.0.0.0/0']] | [0]" \
            --output text)
          if [ "$RULE_EXISTS" == "None" ]; then
            echo "Ingress rule does not exist. Adding rule..."
            aws ec2 authorize-security-group-ingress \
              --group-id ${{ env.SG_ID }} \
              --protocol all \
              --port -1 \
              --cidr 0.0.0.0/0
          else
            echo "Ingress rule already exists. Skipping authorization."
          fi

      - name: Check if EKS Cluster Exists
        id: check-cluster
        run: |
          if aws eks describe-cluster --name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }} > /dev/null 2>&1; then
            echo "Cluster exists."
            echo "cluster_exists=true" >> $GITHUB_ENV
          else
            echo "Cluster does not exist."
            echo "cluster_exists=false" >> $GITHUB_ENV

      - name: Create EKS Cluster
        if: env.cluster_exists == 'false'
        run: |
          aws eks create-cluster \
            --name ${{ env.CLUSTER_NAME }} \
            --role-arn arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/LabRole \
            --resources-vpc-config subnetIds=subnet-0807f6b41b9cb485b,subnet-0cabc9014558f2308,subnet-06f435aa36848d08c,securityGroupIds=${{ env.SG_ID }},endpointPublicAccess=true,endpointPrivateAccess=true,publicAccessCidrs="0.0.0.0/0" \
            --kubernetes-version 1.29 \
            --logging '{"clusterLogging":[{"types":["api","audit","authenticator","controllerManager","scheduler"],"enabled":false}]}'

      - name: Wait for EKS Cluster to be Active
        if: env.cluster_exists == 'false'
        run: |
          aws eks wait cluster-active --name ${{ env.CLUSTER_NAME }} --region ${{ env.AWS_REGION }}

      - name: Check if EKS Node Group Exists
        id: check-nodegroup
        run: |
          if aws eks describe-nodegroup --cluster-name ${{ env.CLUSTER_NAME }} --nodegroup-name ${{ env.NODEGROUP_NAME }} > /dev/null 2>&1; then
            echo "Node Group exists."
            echo "nodegroup_exists=true" >> $GITHUB_ENV
          else
            echo "Node Group does not exist."
            echo "nodegroup_exists=false" >> $GITHUB_ENV

      - name: Create EKS Node Group
        if: env.nodegroup_exists == 'false'
        run: |
          aws eks create-nodegroup \
            --cluster-name ${{ env.CLUSTER_NAME }} \
            --nodegroup-name ${{ env.NODEGROUP_NAME }} \
            --subnets subnet-0807f6b41b9cb485b subnet-0cabc9014558f2308 subnet-06f435aa36848d08c \
            --node-role arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/LabRole \
            --scaling-config minSize=1,maxSize=5,desiredSize=2 \
            --instance-types t3.medium \
            --ami-type AL2_x86_64 \
            --disk-size 20 \
            --capacity-type ON_DEMAND \
            --update-config maxUnavailable=1

      - name: Wait for EKS Node Group to be Active
        if: env.nodegroup_exists == 'false'
        run: |
          aws eks wait nodegroup-active --cluster-name ${{ env.CLUSTER_NAME }} --nodegroup-name ${{ env.NODEGROUP_NAME }} --region ${{ env.AWS_REGION }}


  deploy-application:
    runs-on: ubuntu-latest
    needs: create-cluster
    steps:
      - uses: actions/checkout@v4

      - name: Install kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/v1.30.2/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/
          kubectl version --client
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{env.AWS_REGION}}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}

      - name: Update kube config
        run: aws eks update-kubeconfig --name $CLUSTER_NAME --region $AWS_REGION

      - name: Apply Kubernetes configuration
        run: |
          kubectl apply -f pods/svc-quick-serve-api.yaml
          kubectl apply -f pods/configmap-quick-serve-api.yaml
          kubectl apply -f pods/configmap-quick-serve-db.yaml
          kubectl apply -f pods/secret-quick-serve-api.yaml
          kubectl apply -f pods/secret-quick-serve-db.yaml
          kubectl apply -f pods/quick-serve-api.yaml
          
      - name: Verify Deployment
        run: kubectl get pods,svc,configmaps,secrets,deployments,pv,pvc,hpa
        
